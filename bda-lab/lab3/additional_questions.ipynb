{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "KAFKA_BROKER = 'localhost:9092'\n",
    "KAFKA_TOPIC = 'resumes_topic'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories\n",
    "CATEGORIES = {\n",
    "    'Big Data': ['Hadoop', 'Spark', 'Kafka', 'Flume'],\n",
    "    'NLP': ['NLTK', 'spaCy', 'BERT', 'GPT'],\n",
    "    'AI': ['TensorFlow', 'PyTorch', 'Keras', 'Scikit-learn']\n",
    "}\n",
    "\n",
    "def categorize_resume(resume_text):\n",
    "    categories = {category: [] for category in CATEGORIES}\n",
    "    for category, skills in CATEGORIES.items():\n",
    "        for skill in skills:\n",
    "            if skill.lower() in resume_texow', 'PyTorch', 'Keras', 'Scikit-learn']\n",
    "}t.lower():\n",
    "                categories[category].append(skill)\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Resumes have been processed and sent to Kafka topic `resumes_topic`.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simulated resume data\n",
    "resumes = [\n",
    "    \"Experienced with Hadoop and Spark. Familiar with TensorFlow.\",\n",
    "    \"Skilled in NLTK and BERT. Knowledgeable in PyTorch.\",\n",
    "    \"Expert in Kafka and Flume. Proficient with Keras.\"\n",
    "]\n",
    "\n",
    "for resume in resumes:\n",
    "    categorized_skills = categorize_resume(resume)\n",
    "    producer.send(KAFKA_TOPIC, categorized_skills)\n",
    "\n",
    "producer.flush()\n",
    "display(Markdown(\"**Resumes have been processed and sent to Kafka topic `resumes_topic`.**\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Messages from Consumer Group 1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- {'Big Data': ['Hadoop', 'Spark'], 'NLP': [], 'AI': ['TensorFlow']}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consumer for Group 1\n",
    "consumer_group_1 = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    group_id='consumer_group_1',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "def consume_messages(consumer, group_name):\n",
    "    display(Markdown(f\"**Messages from {group_name}:**\"))\n",
    "    for message in consumer:\n",
    "        display(Markdown(f\"- {message.value}\"))\n",
    "        break \n",
    "\n",
    "consume_messages(consumer_group_1, 'Consumer Group 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Messages from Consumer Group 2:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consumer for Group 2\n",
    "consumer_group_2 = KafkaConsumer(\n",
    "    KAFKA_TOPIC,\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    group_id='consumer_group_2',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Simulate three consumers by running this cell multiple times\n",
    "consume_messages(consumer_group_2, 'Consumer Group 2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
